{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting models - Classification\n",
    "https://machinelearningmastery.com/precision-recall-and-f-measure-for-imbalanced-classification/\n",
    "\n",
    "https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/\n",
    "\n",
    "* Step 1: Are you solving an imbalanced or a binary classification problem?\n",
    "    - Binary: you are predicting Yes/No, On/Off, True/False\n",
    "    - Multi-Class: you are predicting Red/Amber/Green or any 3+ member set\n",
    "\n",
    "* Step 2: Define how your training data represents \"reality\"\n",
    "    - Binary classification\n",
    "        - If you have 98% True and 2% False, False is the **minority class**\n",
    "        - Your model may have difficulty weighting as a result: suggest you create a 50/50 split for training data\n",
    "    - Multi-Class classification\n",
    "        - Same holds true - if you have 3 classes but your minority class is 1%, create a 33/33/33 split for training data\n",
    "\n",
    "Regardless of whether you are doing Binary or Multi-class classification, if your dataset is \"imbalanced\", you will likely get better results if you create a balanced copy of it before continuing\n",
    "   \n",
    "* Step 3: Define the most important metrics for your balance type\n",
    "Assumption is that you have a training set that is evenly split among rows for your predictors (i.e. 50/50 split).\n",
    "   - Binary: \n",
    "       - Do use: Accuracy (general \"feel\" for how good the model is), Recall\n",
    "       - Accuracy and F1\n",
    "       - Don't use: Precision (ratio of \"True positive to total positive predictions\") isn't as helpful in binary with a 50/50 dataset\n",
    "   - Imbalanced: \n",
    "       - Precision tells you the \"accuracy for the minority class\"\n",
    "       - Precision or Recall, possibly F1\n",
    "\n",
    "> Type I Error: False positive (rejection of a true null hypothesis)\n",
    "> Type II Error: False negative (non-rejection of a false null hypothesis)\n",
    "    \n",
    "### Accuracy \n",
    "* Formula: Correct Predictions / Total Predictions (a.k.a. (TP+TN)/(TP+FP+FN+TN)\n",
    "* Meaning: Ratio of correct to total predictions\n",
    "* Use when: Probably your go to for most situations. You would use Recall or Precision if costs of being wrong are very high\n",
    "    - \n",
    "\n",
    "### Precision\n",
    "* Formula: TP/TP+FP\n",
    "* Meaning: \"What % of the total Positive predictions were right (i.e. True Positive)?\"\n",
    "    - \"When the model predicts positive, how often is it correct?\"\n",
    "* ELI5: For an imbalanced data set, this calculates the accuracy for the minority class\n",
    "* Use when: There is a high cost associated with a False Positive\n",
    "    - Example - email spam detection: Use Precision bc the cost is high of a user missing an important email b/c it was falsely marked as spam \n",
    "\n",
    "### Recall\n",
    "* Formula: TP/TP+FN\n",
    "* Meaning: \"When the model predicted correctly, what percent were Positive predictions?\"\n",
    "* Use when: When there is a high cost associated with a False Negative\n",
    "    - Example - detect incoming nuclear missiles: a false negative here kills everyone so use Recall\n",
    "    - Example - fraud detection: use Recall due to the high cost of predicting \"False\" when it was really fraud (i.e. \"True\")\n",
    "    - Example - sick patient detection: use Recall due to the high cost of predicting \"False\" when patient was actually sick (i.e. \"True\")\n",
    "        \n",
    "### F1 Score\n",
    "* Formula: 2* ((Recall * Precision) / (Recall + Precision))\n",
    "* Meaning: It is a balance of Recall and Precision. \n",
    "* ELI5: \"aA good F1 score means that you have low false positives and low false negatives, so youâ€™re correctly identifying real threats and you are not disturbed by false alarms.\"\n",
    "* Use when: you want a balance of Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
